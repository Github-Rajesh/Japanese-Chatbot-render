services:
  backend:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: japanese-chatbot-backend
    restart: unless-stopped
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - HOST=0.0.0.0
      - PORT=8000
      - ENVIRONMENT=production
    volumes:
      # Persist vectorstore data
      - ./data:/app/data
      # Persist knowledge base (PDFs) - handle directory name with spaces
      - "./knowledge base main:/app/knowledge base main"
      # Optional: Mount .env file (if it exists)
      # Note: .env can be provided via environment variables in docker-compose.yml
      # or mounted as a volume. The volume mount below is optional.
      # - ./.env:/app/.env:ro
    ports:
      - "8000:8000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - chatbot-network

  nginx:
    build:
      context: .
      dockerfile: Dockerfile.nginx
    container_name: japanese-chatbot-nginx
    restart: unless-stopped
    ports:
      - "80:80"
    depends_on:
      - backend
    networks:
      - chatbot-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Optional: Ollama service for RakutenAI (if not running externally)
  # Uncomment if you want to run Ollama in Docker
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: japanese-chatbot-ollama
  #   restart: unless-stopped
  #   volumes:
  #     - ollama-data:/root/.ollama
  #   ports:
  #     - "11434:11434"
  #   networks:
  #     - chatbot-network

networks:
  chatbot-network:
    driver: bridge

# volumes:
#   ollama-data:

